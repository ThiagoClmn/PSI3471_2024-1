{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# EP - PSI3471 (2024 - $1^o$ semestre)\n","\n","|Nome do aluno                     |NUSP    |E-mail USP       |\n","|----------------------------------|--------|-----------------|\n","|Gustavo Henrique da Silva Amaral  |12551686|gustavo.amaral7@usp.br|\n","|Thiago da Rocha Calomino Gonçalves|12554647|thcalomino@usp.br|\n","\n","Prof. Hae Yong Kim\n","\n","Link: [Enunciado do EP 2024](http://www.lps.usp.br/hae/psi3471/ep1-2024/ep.pdf)"],"metadata":{"id":"LE8h6eDMDA9W"}},{"cell_type":"markdown","source":["## Instalação de dependências"],"metadata":{"id":"0R9yg5LnNC59"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import glob\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","from collections import Counter\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"],"metadata":{"id":"jGJ0ua51NGbU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importação das imagens do Kaggle"],"metadata":{"id":"2gRCvqg59QwF"}},{"cell_type":"code","source":["# Importar e configurar kaggle\n","!pip3 install -q kaggle\n","!mkdir -p ~/.kaggle\n","!echo '{\"username\":\"thiagocalomino\",\"key\":\"295190f2a38c974aa9e5613554a8c124\"}' > ~/.kaggle/kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# Baixar o dataset de pistache\n","!kaggle datasets download -d muratkokludataset/pistachio-image-dataset\n","\n","# Descompactar o dataset\n","!unzip -q pistachio-image-dataset.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2q0bXxg6Buw","outputId":"06faba50-8771-46f8-9899-1f429d36b9fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/muratkokludataset/pistachio-image-dataset\n","License(s): CC-BY-NC-SA-4.0\n","Downloading pistachio-image-dataset.zip to /content\n"," 94% 25.0M/26.7M [00:00<00:00, 86.0MB/s]\n","100% 26.7M/26.7M [00:00<00:00, 75.3MB/s]\n"]}]},{"cell_type":"markdown","source":["# Redução e alinhamento horizontal das imagens"],"metadata":{"id":"rKABOfVk9YA_"}},{"cell_type":"code","source":["def find_center_and_orientation(image):\n","\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Escala de cinza\n","    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)  # Converte em imagem binária\n","\n","    moments = cv2.moments(binary)  # Momentos da imagem binária\n","    cen_x = moments['m10'] / moments['m00']  # Coord. X do centro de massa\n","    cen_y = moments['m01'] / moments['m00']  # Coord. Y do centro de massa\n","    theta = 0.5 * np.arctan2(2 * moments['mu11'], moments['mu20'] - moments['mu02'])  # Calcula a orientação\n","\n","    return (cen_x, cen_y, theta)  # Retorna o centro de massa e a orientação\n","\n","def reduce_and_align_image(image, size):\n","    reduced_image = cv2.resize(image, (size, size), interpolation=cv2.INTER_AREA)  # Redução para dimensão dada\n","    cen_x, cen_y, theta = find_center_and_orientation(reduced_image)  # Encontra o centro e a orientação da imagem reduzida\n","    height, width = reduced_image.shape[:2]\n","    M = cv2.getRotationMatrix2D((cen_x, cen_y), np.degrees(theta), 1)  # Calcula a matriz de rotação para alinhar a imagem horizontalmente\n","    aligned_image = cv2.warpAffine(reduced_image, M, (width, height), flags=cv2.INTER_LINEAR)  # Aplica a rotação para alinhar a imagem\n","\n","    return reduced_image, aligned_image  # Retorna a imagem reduzida e alinhada\n","\n","def process_images(input_dir, output_dir, size=128):\n","    reduced_dir = os.path.join(output_dir, 'R')  # Diretório para salvar as imagens reduzidas\n","    aligned_dir = os.path.join(output_dir, 'A')  # Diretório para salvar as imagens alinhadas\n","\n","    if not os.path.exists(reduced_dir):\n","        os.makedirs(reduced_dir)  # Cria o diretório para imagens reduzidas se não existir\n","    if not os.path.exists(aligned_dir):\n","        os.makedirs(aligned_dir)  # Cria o diretório para imagens alinhadas se não existir\n","\n","    images = glob.glob(f\"{input_dir}/*.jpg\")  # Lista todas as imagens .jpg no diretório de entrada\n","    print(f\"Encontradas {len(images)} imagens.\")\n","\n","    for image_path in images:\n","        image = cv2.imread(image_path)  # Lê a imagem\n","\n","        reduced_image, aligned_image = reduce_and_align_image(image, size)  # Processa a imagem para reduzir e alinhar\n","        base_name = os.path.basename(image_path)\n","\n","        reduced_image_path = os.path.join(reduced_dir, f\"R_{base_name}\")  # Caminho para salvar a imagem reduzida\n","        aligned_image_path = os.path.join(aligned_dir, f\"A_{base_name}\")  # Caminho para salvar a imagem alinhada\n","\n","        cv2.imwrite(reduced_image_path, reduced_image)  # Salva a imagem reduzida\n","        cv2.imwrite(aligned_image_path, aligned_image)  # Salva a imagem alinhada"],"metadata":{"id":"HjZApCcdO1HM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Redução e alinhamento das imagens de pistache\n","process_images('Pistachio_Image_Dataset/Pistachio_Image_Dataset/Kirmizi_Pistachio', 'processed_images', size=128)\n","process_images('Pistachio_Image_Dataset/Pistachio_Image_Dataset/Siirt_Pistachio', 'processed_images', size=128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lg3suRDf4_0A","outputId":"07a92bf9-9b2c-4136-c43e-52ffafbc0b2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encontradas 1232 imagens.\n","Encontradas 916 imagens.\n"]}]},{"cell_type":"markdown","source":["# Função para carregar os dados"],"metadata":{"id":"NaCVPn1X-HFQ"}},{"cell_type":"code","source":["def load_dataset(directory):\n","    data = []\n","    labels = []\n","    print(f\"Carregando imagens do diretório: {directory}\")\n","\n","    for filename in os.listdir(directory):  # Itera sobre todos os arquivos no diretório\n","        if filename.endswith(\".jpg\"):  # Verifica se o arquivo é uma imagem .jpg\n","\n","            # Verifica o nome do grão e define o seu respectivo rótulo\n","            if 'kirmizi' in filename.lower():\n","                label = 'kirmizi'\n","            elif 'siirt' in filename.lower():\n","                label = 'siirt'\n","\n","            image_path = os.path.join(directory, filename)\n","            image = load_img(image_path, target_size=(128, 128))  # Carrega a imagem e redimensiona para 128 x 128\n","            image = img_to_array(image)  # Converte para NumPy Array\n","            data.append(image)\n","            labels.append(label)\n","\n","    print(f\"Total de imagens carregadas: {len(data)}\")\n","\n","    data = np.array(data, dtype=\"float\") / 255.0  # Normaliza os valores dos pixels para o intervalo [0, 1]\n","    le = LabelEncoder()\n","    labels = le.fit_transform(labels)  # Converte os rótulos para valores inteiros\n","    return data, labels, le"],"metadata":{"id":"D1JkvGzG-XOj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implementação por SVM"],"metadata":{"id":"rt8Ke25w-xy8"}},{"cell_type":"code","source":["def classify_images(data, labels):\n","    class_distribution = Counter(labels)  # Conta a distribuição das classes\n","    print(f\"Distribuição das classes: {class_distribution}\")\n","\n","    trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.5, stratify=labels)  # Divide os dados em treino e teste\n","\n","    # Escalonamento dos dados\n","    scaler = StandardScaler()\n","    trainX = scaler.fit_transform(trainX.reshape(trainX.shape[0], -1))  # Ajusta e transforma os dados de treino\n","    testX = scaler.transform(testX.reshape(testX.shape[0], -1))  # Transforma os dados de teste usando os parâmetros calculados nos dados de treino\n","\n","    model = SVC(kernel='linear', C=1)  # Define o modelo SVM com kernel linear e C=1\n","    model.fit(trainX, trainY)  # Treina o modelo\n","    predictions = model.predict(testX)  # Faz previsões nos dados de teste\n","    accuracy = accuracy_score(testY, predictions)  # Calcula a acurácia das previsões\n","    return accuracy"],"metadata":{"id":"u0_YMkoYO-Gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Carregar e classificar imagens reduzidas\n","dataR, labelsR, leR = load_dataset('processed_images/R')\n","print(f\"Classes disponíveis: {leR.classes_}\")\n","reduced_accuracy = classify_images(dataR, labelsR)\n","print(f\"Taxa de acerto (imagens reduzidas): {100*reduced_accuracy} %\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wr9WYZfj5o66","outputId":"4be67be4-edd6-457c-992e-30105f1eefe7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Carregando imagens do diretório: processed_images/R\n","Total de imagens carregadas: 2148\n","Classes disponíveis: ['kirmizi' 'siirt']\n","Distribuição das classes: Counter({0: 1232, 1: 916})\n","Taxa de acerto (imagens reduzidas): 83.9851024208566 %\n"]}]},{"cell_type":"code","source":["# Carregar e classificar imagens alinhadas\n","dataA, labelsA, leA = load_dataset('processed_images/A')\n","print(f\"Classes disponíveis: {leA.classes_}\")\n","aligned_accuracy = classify_images(dataA, labelsA)\n","print(f\"Taxa de acerto (imagens alinhadas): {100 * aligned_accuracy} %\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGuXKaIut_wU","outputId":"746bee58-5152-4a72-ea39-32e27315297c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Carregando imagens do diretório: processed_images/A\n","Total de imagens carregadas: 2148\n","Classes disponíveis: ['kirmizi' 'siirt']\n","Distribuição das classes: Counter({0: 1232, 1: 916})\n","Taxa de acerto (imagens alinhadas): 82.68156424581005 %\n"]}]},{"cell_type":"markdown","source":["# Implementação por CNN"],"metadata":{"id":"osY9CIShAB6d"}},{"cell_type":"code","source":["def create_cnn_model(input_shape):\n","    model = Sequential([\n","        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),  # Primeira camada convolucional\n","        MaxPooling2D((2, 2)),  # Primeira camada de pooling\n","        Conv2D(64, (3, 3), activation='relu'),  # Segunda camada convolucional\n","        MaxPooling2D((2, 2)),  # Segunda camada de pooling\n","        Conv2D(128, (3, 3), activation='relu'),  # Terceira camada convolucional\n","        MaxPooling2D((2, 2)),  # Terceira camada de pooling\n","        Flatten(),  # Achata a saída das camadas convolucionais para uma única dimensão\n","        Dense(256, activation='relu'),  # Primeira camada totalmente conectada com 256 neurônios\n","        Dense(128, activation='relu'),  # Segunda camada totalmente conectada com 128 neurônios\n","        Dense(2, activation='softmax')  # Camada de saída com 2 neurônios para as classes 'kirmizi' e 'siirt'\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Compila o modelo\n","    return model\n","\n","def classify_images_with_cnn(data, labels):\n","    class_distribution = Counter(labels)  # Conta a distribuição das classes\n","    print(f\"Distribuição das classes: {class_distribution}\")\n","\n","    trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.5, stratify=labels)  # Divide os dados em treino e teste\n","\n","    model = create_cnn_model(input_shape=(128, 128, 3)) # Cria o modelo de CNN\n","    model.fit(trainX, trainY, batch_size=32, epochs=10, verbose=1)  # Treina o modelo\n","    loss, accuracy = model.evaluate(testX, testY)  # Avalia o modelo nos dados de teste\n","    return accuracy"],"metadata":{"id":"grpyiHuErd-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Classificar imagens reduzidas com CNN\n","print(f\"Classes disponíveis: {leR.classes_}\")\n","reduced_accuracy = classify_images_with_cnn(dataR, labelsR)\n","print(f\"Taxa de acerto CNN (imagens reduzidas): {100 * reduced_accuracy} %\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3gqKSF0i5tUK","outputId":"e0449daa-f32f-4599-be2b-93561b8f1c28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classes disponíveis: ['kirmizi' 'siirt']\n","Distribuição das classes: Counter({0: 1232, 1: 916})\n","Epoch 1/10\n","34/34 [==============================] - 7s 33ms/step - loss: 0.5469 - accuracy: 0.7393\n","Epoch 2/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.3990 - accuracy: 0.8128\n","Epoch 3/10\n","34/34 [==============================] - 1s 20ms/step - loss: 0.3868 - accuracy: 0.8352\n","Epoch 4/10\n","34/34 [==============================] - 1s 21ms/step - loss: 0.3390 - accuracy: 0.8566\n","Epoch 5/10\n","34/34 [==============================] - 1s 21ms/step - loss: 0.3128 - accuracy: 0.8687\n","Epoch 6/10\n","34/34 [==============================] - 1s 20ms/step - loss: 0.3076 - accuracy: 0.8706\n","Epoch 7/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.2860 - accuracy: 0.8855\n","Epoch 8/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.2361 - accuracy: 0.9032\n","Epoch 9/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.2058 - accuracy: 0.9125\n","Epoch 10/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.1774 - accuracy: 0.9264\n","34/34 [==============================] - 0s 8ms/step - loss: 0.4360 - accuracy: 0.8361\n","Taxa de acerto CNN (imagens reduzidas): 83.61266255378723 %\n"]}]},{"cell_type":"code","source":["# Classificar imagens alinhadas com CNN\n","print(f\"Classes disponíveis: {leA.classes_}\")\n","aligned_accuracy = classify_images_with_cnn(dataA, labelsA)\n","print(f\"Taxa de acerto CNN (imagens alinhadas): {100 * aligned_accuracy} %\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZ-Q94H_uCeZ","outputId":"9ecbd5d4-02bf-4c2f-dbeb-e5646a6ae695"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classes disponíveis: ['kirmizi' 'siirt']\n","Distribuição das classes: Counter({0: 1232, 1: 916})\n","Epoch 1/10\n","34/34 [==============================] - 2s 21ms/step - loss: 0.4930 - accuracy: 0.7542\n","Epoch 2/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.3944 - accuracy: 0.8110\n","Epoch 3/10\n","34/34 [==============================] - 1s 19ms/step - loss: 0.3412 - accuracy: 0.8464\n","Epoch 4/10\n","34/34 [==============================] - 1s 22ms/step - loss: 0.2984 - accuracy: 0.8696\n","Epoch 5/10\n","34/34 [==============================] - 1s 22ms/step - loss: 0.2915 - accuracy: 0.8669\n","Epoch 6/10\n","34/34 [==============================] - 1s 22ms/step - loss: 0.2440 - accuracy: 0.8939\n","Epoch 7/10\n","34/34 [==============================] - 1s 19ms/step - loss: 0.2398 - accuracy: 0.8929\n","Epoch 8/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.2014 - accuracy: 0.9209\n","Epoch 9/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.1519 - accuracy: 0.9330\n","Epoch 10/10\n","34/34 [==============================] - 1s 18ms/step - loss: 0.1328 - accuracy: 0.9451\n","34/34 [==============================] - 0s 7ms/step - loss: 0.3157 - accuracy: 0.8771\n","Taxa de acerto CNN (imagens alinhadas): 87.70949840545654 %\n"]}]}]}