{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOAGMMt+91T0AxuZnpEKQGw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# EP - PSI3471 (2024 - $1^o$ semestre)\n","\n","|Nome do aluno                     |NUSP    |E-mail USP       |\n","|----------------------------------|--------|-----------------|\n","|Gustavo Henrique da Silva Amaral  |12551686|gustavo.amaral7@usp.br|\n","|Thiago da Rocha Calomino Gonçalves|12554647|thcalomino@usp.br|\n","\n","Link: [Enunciado do EP 2024](http://www.lps.usp.br/hae/psi3471/ep1-2024/index.html)"],"metadata":{"id":"GLWN83jWhCeb"}},{"cell_type":"markdown","source":["# Instalação de dependências"],"metadata":{"id":"Mm53GiRwhOUX"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"Kec_AWuygq77","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719770859740,"user_tz":180,"elapsed":26149,"user":{"displayName":"Thiago da Rocha Calomino Goncalves","userId":"12806470346850057124"}},"outputId":"efb97d5b-90f7-4adf-9fa2-77c91f4e9d7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opendatasets\n","  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.4)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.14)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.7)\n","Installing collected packages: opendatasets\n","Successfully installed opendatasets-0.1.22\n"]}],"source":["!pip install opendatasets\n","\n","import sys\n","import time\n","import os\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","import seaborn as sns\n","\n","import pandas as pd\n","import opendatasets as od\n","import cv2\n","\n","import tensorflow.keras as keras\n","from keras.datasets import mnist\n","from sklearn import tree\n","from sklearn.metrics import confusion_matrix\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#import requests\n","#from PIL import Image\n","#from io import BytesIO"]},{"cell_type":"code","source":["import cv2\n","import glob\n","import os\n","\n","def reduce_image(image_path, output_size):\n","    image = cv2.imread(image_path)\n","    reduced_image = cv2.resize(image, (output_size, output_size), interpolation=cv2.INTER_AREA)\n","    return reduced_image\n","\n","def save_image(image, output_path):\n","    cv2.imwrite(output_path, image)\n","\n","input_images = glob.glob(\"path_to_images/*.jpg\")\n","output_size = 100  # Por exemplo, reduzindo para 100x100 pixels\n","\n","for image_path in input_images:\n","    reduced_image = reduce_image(image_path, output_size)\n","    output_path = os.path.join(\"path_to_output\", f\"R_{os.path.basename(image_path)}\")\n","    save_image(reduced_image, output_path)\n"],"metadata":{"id":"nWeSEjiqUt-Y","executionInfo":{"status":"ok","timestamp":1719770865622,"user_tz":180,"elapsed":301,"user":{"displayName":"Thiago da Rocha Calomino Goncalves","userId":"12806470346850057124"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def findCenterAndOrientation(image):\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n","    moments = cv2.moments(binary)\n","    cen_x = moments['m10'] / moments['m00']\n","    cen_y = moments['m01'] / moments['m00']\n","    theta = 0.5 * np.arctan2(2 * moments['mu11'], moments['mu20'] - moments['mu02'])\n","    return cen_x, cen_y, theta\n","\n","def align_image(image):\n","    cen_x, cen_y, theta = findCenterAndOrientation(image)\n","    (h, w) = image.shape[:2]\n","    center = (w // 2, h // 2)\n","    M = cv2.getRotationMatrix2D(center, np.degrees(theta), 1.0)\n","    aligned_image = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n","    return aligned_image\n","\n","for image_path in input_images:\n","    reduced_image = reduce_image(image_path, output_size)\n","    aligned_image = align_image(reduced_image)\n","    output_path = os.path.join(\"path_to_output\", f\"A_{os.path.basename(image_path)}\")\n","    save_image(aligned_image, output_path)\n"],"metadata":{"id":"FiEoGsE_UwnF","executionInfo":{"status":"ok","timestamp":1719770867310,"user_tz":180,"elapsed":6,"user":{"displayName":"Thiago da Rocha Calomino Goncalves","userId":"12806470346850057124"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","def load_images(image_paths):\n","    images = [cv2.imread(image_path) for image_path in image_paths]\n","    return np.array([cv2.resize(image, (output_size, output_size)).flatten() for image in images])\n","\n","kirmizi_images = glob.glob(\"path_to_output/R_kirmizi*.jpg\")\n","siirt_images = glob.glob(\"path_to_output/R_siirt*.jpg\")\n","\n","kirmizi_labels = [0] * len(kirmizi_images)\n","siirt_labels = [1] * len(siirt_images)\n","\n","all_images = kirmizi_images + siirt_images\n","all_labels = kirmizi_labels + siirt_labels\n","\n","X = load_images(all_images)\n","y = np.array(all_labels)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n","\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"sqGZp-VpUyjW","executionInfo":{"status":"error","timestamp":1719770869282,"user_tz":180,"elapsed":330,"user":{"displayName":"Thiago da Rocha Calomino Goncalves","userId":"12806470346850057124"}},"outputId":"79968585-837a-46d5-9afa-87f80609bc56"},"execution_count":5,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"With n_samples=0, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-8a54aeb32e44>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","def load_images_for_cnn(image_paths):\n","    images = [cv2.imread(image_path) for image_path in image_paths]\n","    return np.array([cv2.resize(image, (output_size, output_size)) for image in images])\n","\n","X = load_images_for_cnn(all_images)\n","y = to_categorical(all_labels, num_classes=2)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n","\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(output_size, output_size, 3)),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dense(2, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n"],"metadata":{"id":"1T8YTXTxU18K"},"execution_count":null,"outputs":[]}]}